{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5869783-46cd-4ef5-8d80-b1c6eaf559bd"
   },
   "source": [
    "# Xử lý ngôn ngữ tự nhiên và khai phá dữ liệu văn bản (`NLP`)\n",
    "\n",
    "![](https://www.thuatngumarketing.com/wp-content/uploads/2017/12/NLP.png.pagespeed.ce_.1YNuw_5dJH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "372869b4-6785-4aaa-8abc-3e29d875fb0c"
   },
   "source": [
    "## Định nghĩa\n",
    "\n",
    "> **Xử lí ngôn ngữ tự nhiên**: Sử dụng các kỹ thuật phân tích và làm sạch dữ liệu kết hợp với mô hình học máy để khai thác thông tin trong dữ liệu ngôn ngữ.\n",
    "\n",
    "> **Dữ liệu ngôn ngữ**: Nói cho nhau nghe (dữ liệu ngôn ngữ âm thanh) và viết cho nhau đọc (dữ liệu ngôn ngữ văn bản)\n",
    "\n",
    "> **Text Mining**: Kỹ thuật xử lý dữ liệu dạng văn bản và khai thác thông tin từ dữ liệu văn bản.\n",
    "\n",
    "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0108.png)\n",
    "\n",
    "## Ứng dụng\n",
    "\n",
    "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0101.png)\n",
    "\n",
    "> Tham khảo: [Practical Natural Language Processing by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, Harshit Surana\n",
    "](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da1ff90f-7126-4e50-8272-e7bb5ebc5092"
   },
   "source": [
    "## Quy trình\n",
    "\n",
    "> 1. **Preprocessing** (Tiền xử lý) (Làm sạch & Chuẩn hoá dữ liệu)\n",
    "> 2. **Tokenizing** (Tách từ)\n",
    "> 3. **Vectorizing** (Vectơ hóa)\n",
    "> 4. **Modeling** (Xây dựng mô hình)\n",
    "> 5. **Intepreting result & Application** (Ứng dụng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools support us:\n",
    "> - gợi ý viết code (note: chỉ dùng được cho VSCode): https://copilot.github.com/\n",
    "> - code sẵn, cải tiến hơn Git_copilot: https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4de5b914-9acd-4e9f-9072-a6663fea3313"
   },
   "source": [
    "## Công cụ và thư viện\n",
    "\n",
    "1. Python Nature Language Toolkit ([Python NLTK](https://www.nltk.org/)), PyVi hỗ trợ xử lý dữ liệu dạng văn bản.\n",
    "2. Gensim/Tensorflow/Scikit-learn hỗ trợ xây dựng mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4da8ab72-48f2-4e8d-b889-1f8637c81797"
   },
   "source": [
    "#### 1. Tiền xử lý dữ liệu text (Text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Today is our   last     lesson ! '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '    Today is our   last     lesson ! '\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'our', 'last', 'lesson', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is our last lesson !'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is our   last     lesson !'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is our   last     lesson ! '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Today is our   last     lesson !'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    today is our   last     lesson ! '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.lower()                                                 # viết thường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    TODAY IS OUR   LAST     LESSON ! '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.upper()                                                 # viết in hoa toàn bộ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is great'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'today is great'.capitalize()                                  # viết hoa chữ cái đầu tiên của câu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Today Is Our   Last     Lesson ! '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.title()                                                 # viết hoa chữ cái đầu của mỗi word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "651749cb-7a5f-4843-b816-67b5a34effde"
   },
   "source": [
    "#### TODO: Làm sạch dữ liệu chỉ giữ lại dữ liệu bằng chữ\n",
    "1. Nếu là dữ liệu scrapped từ web thì chỉ trích lọc dữ liệu chữ (text only)\n",
    "2. Loại bỏ hyperlink (nếu có) # https://baophapluat.com # lẫn thành từ word \n",
    "3. Nếu là dữ liệu web thì cần loại bỏ emoji # :smile: :angry:\n",
    "4. Loại bỏ tất cả các dấu (.,\\/!@#$%^&*()+_ etc.)\n",
    "```\n",
    "r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\“\\”\\\\\\-\\:…&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\•]'\n",
    "```\n",
    "5. Loại bỏ tất cả các số\n",
    "6. Loại bỏ các khoảng trắng và đổi text thành lowercase # normalize # T == t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''<head></head>\n",
    "<body>\n",
    "    <p>\n",
    "    Here's 1 paragraph of text 🔥. !\n",
    "    <a href=\"https://www.dataquest.io\">Learn Data Science Online 🤡</a>\n",
    "    </p>\n",
    "    <p>\n",
    "    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\n",
    "    <a href=\"https://www.python.org\">Python 101</a> </p>\n",
    "</body></html>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# 1. Trích lọc Data dạng Text khỏi thẻ html:\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head>\n",
       "<body>\n",
       "<p>\n",
       "    Here's 1 paragraph of text 🔥. !\n",
       "    <a href=\"https://www.dataquest.io\">Learn Data Science Online 🤡</a>\n",
       "</p>\n",
       "<p>\n",
       "    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\n",
       "    <a href=\"https://www.python.org\">Python 101</a> </p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "bs4.BeautifulSoup(text)                                    # đọc file Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n    Here's 1 paragraph of text 🔥. !\\n    Learn Data Science Online 🤡\\n\\n\\n    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\\n    Python 101 \\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs4.BeautifulSoup(text).get_text('')                       # chỉ lấy phần Text ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_html(text):\n",
    "    return bs4.BeautifulSoup(text).get_text('')                       # chỉ lấy phần Text ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n    Here's 1 paragraph of text 🔥. !\\n    Learn Data Science Online 🤡\\n\\n\\n    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\\n    Python 101 \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_html(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Loại bỏ đường Link:\n",
    "import re\n",
    "\n",
    "def del_link(text):\n",
    "    link = 'http[\\S]*'\n",
    "    text = re.sub(link, ' ', str(text))\n",
    "    text = re.sub('\\n', ' ', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       Here's 1 paragraph of text 🔥. !     Learn Data Science Online 🤡       Here's a 2nd paragraph of text! Further informations at       Python 101  \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_link(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Làm quen với những kí tự trong Text: https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# 3. Loại bỏ Emoji:\n",
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "def del_emoji(text):\n",
    "    return demoji.replace(text, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       Here's 1 paragraph of text . !     Learn Data Science Online        Here's a 2nd paragraph of text! Further informations at       Python 101  \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_emoji(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Loại bỏ dấu:\n",
    "def del_punctuation(text):\n",
    "    Punc = r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\“\\”\\\\\\-\\:…&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\•]'\n",
    "    text = re.sub(Punc, ' ', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       Here s 1 paragraph of text         Learn Data Science Online        Here s a 2nd paragraph of text  Further informations at       Python 101  '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_punctuation(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Loại bỏ số:\n",
    "def del_numbers(text):\n",
    "    return re.sub('\\d+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       Here s   paragraph of text         Learn Data Science Online        Here s a  nd paragraph of text  Further informations at       Python    '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_numbers(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Loại bỏ các khoảng trắng và đổi text thành lowercase # normalize # T == t:\n",
    "def del_space(text):\n",
    "    return re.sub('\\s+', ' ', text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here s paragraph of text learn data science online here s a nd paragraph of text further informations at python'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_space(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Trong Vectors: Num != num != NUm != NUM --> nên chuyển về dạng viết thường (lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Tách từ (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 'a',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'p',\n",
       " 'h',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " ' ',\n",
       " 'd',\n",
       " 'a',\n",
       " 't',\n",
       " 'a',\n",
       " ' ',\n",
       " 's',\n",
       " 'c',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 'a',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'p',\n",
       " 'h',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'u',\n",
       " 'r',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'm',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'p',\n",
       " 'y',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_char(text):                             # character tokenization\n",
    "    return [char for char in text]\n",
    "\n",
    "split_char(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 's',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'science',\n",
       " 'online',\n",
       " 'here',\n",
       " 's',\n",
       " 'a',\n",
       " 'nd',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'further',\n",
       " 'informations',\n",
       " 'at',\n",
       " 'python']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()                                           # word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 's',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'science',\n",
       " 'online',\n",
       " 'here',\n",
       " 's',\n",
       " 'a',\n",
       " 'nd',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'further',\n",
       " 'informations',\n",
       " 'at',\n",
       " 'python']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.word_tokenize(text)                                                          # word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is our last lesson.', 'Wish you all happy learning!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize('Today is our last lesson. Wish you all happy learning!')                                   # sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Học', 'sinh', 'học', 'sinh', 'học']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'Học sinh học sinh học'\n",
    "nltk.word_tokenize(text1)                                # những từ 'học' & 'sinh' mang nhiều nghĩa khác nhau nhưng máy sẽ không hiểu và dễ bị đánh giá sai     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Cách `tokenize` đối với data bằng Vietnamese:  [Underthesea](https://github.com/undertheseanlp/underthesea), or [Pyvi](https://github.com/trungtv/pyvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyvi) (1.0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.22.2)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Học_sinh phải học_sinh_học'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi.ViTokenizer import tokenize\n",
    "tokenize('Học sinh phải học sinh học')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tổng hợp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):                                                    # làm sạch dữ liệu từ đầu tới cuối\n",
    "    text = del_html(text)\n",
    "    text = del_link(text)\n",
    "    text = del_numbers(text)\n",
    "    text = del_emoji(text)\n",
    "    text = del_punctuation(text)\n",
    "    text = del_space(text)\n",
    "    return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>(Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...</td>\n",
       "      <td>Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...</td>\n",
       "      <td>Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...</td>\n",
       "      <td>Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đề xuất tăng liều lượng gói kích thích kinh tế...</td>\n",
       "      <td>Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...</td>\n",
       "      <td>Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tác động từ chính sách tín dụng: BĐS có thể đó...</td>\n",
       "      <td>Động thái thắt chặt hay nới lỏng của chính sác...</td>\n",
       "      <td>Lời tòa soạn Thị trường bất động sản ngày càng...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "1  Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...   \n",
       "2  Đề xuất tăng liều lượng gói kích thích kinh tế...   \n",
       "3  Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "4  Tác động từ chính sách tín dụng: BĐS có thể đó...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  (Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...   \n",
       "1  Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...   \n",
       "2  Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...   \n",
       "3  Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...   \n",
       "4  Động thái thắt chặt hay nới lỏng của chính sác...   \n",
       "\n",
       "                                             content  \n",
       "0  Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...  \n",
       "1  Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...  \n",
       "2  Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...  \n",
       "3  Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...  \n",
       "4  Lời tòa soạn Thị trường bất động sản ngày càng...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('https://github.com/honghanhh/mci_python_36a1_l2/raw/main/Lectures/lecture_09/data.json')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>(Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...</td>\n",
       "      <td>Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...</td>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...</td>\n",
       "      <td>Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...</td>\n",
       "      <td>Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...</td>\n",
       "      <td>Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Đề xuất tăng liều lượng gói kích thích kinh tế...</td>\n",
       "      <td>Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...</td>\n",
       "      <td>Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...</td>\n",
       "      <td>Đề xuất tăng liều lượng gói kích thích kinh tế...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "      <td>Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...</td>\n",
       "      <td>Thủ tướng: Khác với đa số các nước, dư địa chí...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tác động từ chính sách tín dụng: BĐS có thể đó...</td>\n",
       "      <td>Động thái thắt chặt hay nới lỏng của chính sác...</td>\n",
       "      <td>Lời tòa soạn Thị trường bất động sản ngày càng...</td>\n",
       "      <td>Tác động từ chính sách tín dụng: BĐS có thể đó...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "1  Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...   \n",
       "2  Đề xuất tăng liều lượng gói kích thích kinh tế...   \n",
       "3  Thủ tướng: Khác với đa số các nước, dư địa chí...   \n",
       "4  Tác động từ chính sách tín dụng: BĐS có thể đó...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  (Tổ Quốc) - Với bối cảnh hiện nay, Hội đồng Tư...   \n",
       "1  Tại cuộc họp Hội đồng Tư vấn chính sách tài ch...   \n",
       "2  Chuyên gia đề xuất do dịch bệnh Covid-19 còn k...   \n",
       "3  Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...   \n",
       "4  Động thái thắt chặt hay nới lỏng của chính sác...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Sáng ngày 9/7, Hội đồng Tư vấn chính sách tài ...   \n",
       "1  Tham dự cuộc họp có Thống đốc NHNN Lê Minh Hưn...   \n",
       "2  Sáng 9/7, Thủ tướng Nguyễn Xuân Phúc chủ trì h...   \n",
       "3  Với bối cảnh hiện nay, Hội đồng Tư vấn chính s...   \n",
       "4  Lời tòa soạn Thị trường bất động sản ngày càng...   \n",
       "\n",
       "                                              corpus  \n",
       "0  Thủ tướng: Khác với đa số các nước, dư địa chí...  \n",
       "1  Thống nhất kịch bản tăng trưởng 3-4%, lạm phát...  \n",
       "2  Đề xuất tăng liều lượng gói kích thích kinh tế...  \n",
       "3  Thủ tướng: Khác với đa số các nước, dư địa chí...  \n",
       "4  Tác động từ chính sách tín dụng: BĐS có thể đó...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data.apply(lambda x: ' '.join(x), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thủ_tướng khác với đa_số các nước dư địa_chính_sách tài khóa tiền_tệ của việt nam còn khá lớn tổ_quốc với bối_cảnh hiện_nay hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia thống_nhất kịch_bản tăng_trưởng từ kiểm_soát lạm_phát dưới năm và đầu tăng_trưởng tín_dụng trên chủ_trương tăng thêm bội_chi ngân_sách nợ công khoảng gdp sáng ngày hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia đã họp dưới sự chủ_trì của thủ_tướng nguyễn xuân phúc chủ_tịch hội_đồng dư địa_chính_sách tài khóa tiền_tệ còn lớn thủ_tướng nguyễn xuân phúc cho rằng dịch_bệnh covid bùng_phát diện rộng trên toàn_cầu diễn_biến phức_tạp chưa dừng lại nhất_là tại các đối_tác lớn của nước ta kinh_tế toàn_cầu sụt_giảm mạnh nhiều tổ_chức quốc_tế dự_báo kinh_tế thế_giới tăng_trưởng âm trong năm nay trong bối_cảnh đó hầu_hết các nước đều nới lỏng chính_sách tài_chính tiền_tệ với mức_độ chưa từng có theo thống_kê mới nhất nếu tháng tổng_các gói kích_thích tài khóa mới là tỷ usd thì đến nay đã tăng lên tỷ usd chưa có dấu_hiệu dừng lại về tình_hình trong nước theo thủ_tướng chúng_ta là một trong những nước kiểm_soát dịch_bệnh sớm nhất gần tháng qua không có ca lây_nhiễm mới trong cộng_đồng và đang tích_cực thực_hiện mục_tiêu kép hiện_nay các cân_đối lớn của nền kinh_tế đều giữ vững các lĩnh_vực trọng_yếu có xu_hướng đi lên mạnh_mẽ tuy_nhiên thủ_tướng lưu_ý cần nhận_diện rõ rủi_ro đưa ra các biện_pháp điều_hành đồng_bộ hiệu_quả hơn để giữ ổn_định vĩ_mô kiểm_soát lạm_phát dưới khác với đa_số các nước dư địa_chính_sách tài khóa tiền_tệ của chúng_ta còn khá lớn cho kích_thích tổng_cầu thúc_đẩy tăng_trưởng cần chính_sách phục_hồi phát_triển kinh_tế dài_hạn cho cả năm tại cuộc họp các thành_viên hội_đồng nhận_định tình_hình_thế_giới khu_vực tiếp_tục diễn_biến phức_tạp khó lường dịch_bệnh chưa kết_thúc trong năm nay mà có_thể kéo_dài trong thời_gian tới do vậy có ý_kiến cho rằng gói chính_sách phục_hồi phát_triển kinh_tế phải mang tính dài_hạn cho cả năm chứ không chỉ trong năm nay với tinh_thần tiến_công kinh_tế trong khi phòng_thủ dịch_bệnh chuyên_gia bùi đức thụ cho rằng cứu_trợ doanh_nghiệp là việc cần làm nhưng gia_tăng sức_sống cho doanh_nghiệp không_thể chỉ làm trong năm ngoài cơ_chế hỗ_trợ khắc_phục ảnh_hưởng của covid cần có các chính_sách hỗ_trợ đặc_biệt phục_vụ tái_cơ_cấu doanh_nghiệp mạnh hơn không chỉ tái_cơ_cấu thị_trường đầu_vào đầu_tư mà cả lao_động tăng sức chống chịu của nền kinh_tế thực_lực doanh_nghiệp của chúng_ta còn yếu đặc_biệt đa_số là doanh_nghiệp vừa và nhỏ ts trần đình thiên nhận_định do đó không chỉ cứu cái cũ mà còn cần tạo cả cái mới tức_là bên cạnh hỗ_trợ doanh_nghiệp cần thúc_đẩy khởi_nghiệp sáng_tạo trong tình_thế khó_khăn này cần tính tới các biện_pháp xử_lý nợ xấu ts trần đình thiên đề_nghị chính_phủ nhà_nước phải là người mua hàng lớn nhất đối_với các sản_phẩm made in việt nam một_số thành_viên kiến_nghị so với các nước thì gói hỗ_trợ tài khóa của chúng_ta là ít_nhất do đó cần tập_trung vào gói này nhiều hơn cũng như tăng quy_mô các gói hỗ_trợ nhất là cho ngành_hàng không_kích_cầu nội_địa cũng_nên hướng vào kích_cầu du_lịch bán_lẻ tín_dụng tiêu_dùng ngành ngân_hàng cần tiếp_tục hạ_lãi_suất cho vay hội_đồng tư_vấn chính_sách tài_chính tiền_tệ quốc_gia đã thống_nhất kịch_bản tăng_trưởng từ kiểm_soát lạm_phát dưới năm và đầu tăng_trưởng tín_dụng trên chủ_trương tăng thêm bội_chi ngân_sách nợ công khoảng gdp để có thêm nguồn_lực chúng_ta chuẩn_bị sẵn_sàng để có_thể hỗ_trợ doanh_nghiệp kiên_quyết bảo_vệ hệ_thống doanh_nghiệp không để đứt_gãy mất năng_lực sản_xuất trong những ngành lĩnh_vực trọng_yếu ngành ngân_hàng tiếp_tục xem_xét giảm lãi_suất chia_sẻ khó_khăn với doanh_nghiệp và người dân tiếp_tục tiết_kiệm chi thường_xuyên cắt_giảm các khoản chi_hội_nghị hội_thảo dành nguồn_lực cho những nhiệm_vụ cấp_bách chống dịch hỗ_trợ kịp_thời cho người dân doanh_nghiệp'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data['corpus'].apply(clean_text)                   # corpora là corpus: sdung trong Text Mining (thể hiện việc sdung nhiều văn bản Text)\n",
    "data['corpus'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Read more about Tokenization: https://huggingface.co/docs/transformers/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa513f1e-0125-4cdc-a268-883af940025b"
   },
   "source": [
    "#### 3. Mã hóa dữ liệu text (Word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Trong `CountVectorizer` sẽ xử lý data cơ bản, filltering 1 số từ `stopwords` trong tiếng Anh, đối với Vietnamese thì chúng ta phải tự loại bỏ\n",
    "> - `Stopwords` là những từ không có ý nghĩa, mang tính chất làm cho đúng ngữ pháp trong câu, ex: `the, is, about, ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876, 5743)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape                                                       # có dạng (số rows, số Tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TF-IDF` (Term Frequency & Inverse Document Frequency):\n",
    "- `TF`: Mức độ xuất hiện của từ trong văn bản\n",
    "- `IDF`: chia cho tỉ lệ văn bản mà từ đó xuất hiện trên tổng tất cả số lượng văn bản\n",
    "---> Scale được giá trị Vectors về 1 khoảng giá trị nhất định\n",
    "---> Loại bỏ những từ được coi là `Stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer()\n",
    "X_train_tf = tf_transformer.fit(X_train_counts).transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876, 5743)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b347065-59af-414d-8fee-f37389be5b5d"
   },
   "source": [
    "### Khai phá dữ liệu text (Text Mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was the best of times',\n",
       " 'It was the worst of times',\n",
       " 'It was the age of wisdom',\n",
       " 'It was the age of foolishness']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'It was the best of times',                             # best times\n",
    "    'It was the worst of times',                            # worst times\n",
    "    'It was the age of wisdom',                             # age wisdom\n",
    "    'It was the age of foolishness'                         # age foolishness\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b82a86e-d2e8-4c4c-a418-124c90218d1f"
   },
   "source": [
    "#### Khái niệm Bag-of-Words\n",
    "\n",
    "Là kỹ thuật chia văn bản thành các tổ hợp từ khác nhau (bằng phương pháp tokenize). Cách chia phổ biến là mỗi câu thành 1 văn bản (bag) và mỗi văn bản được chia thành từ (word). Dựa vào đó có thể đo lường mức độ xuất hiện của các từ trong văn bản và xây dựng mối liên hệ giữa ngữ cảnh và các từ. \n",
    "\n",
    "Hai yếu tố:\n",
    "1. Từ điển của các từ được sử dụng\n",
    "2. Mức độ xuất hiện của các từ trong từ điển\n",
    "*Mỗi từ hay token được gọi là một `gram`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1637329798399,
     "user": {
      "displayName": "Samuel Doan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbcdQ40FcyCSIyitBN2ptY3zHWA_09harKPYF2=s64",
      "userId": "04384888964432338542"
     },
     "user_tz": -420
    },
    "id": "vKTTqu9ccnHq",
    "outputId": "b3daeb63-d877-4c17-b270-e435c4c76917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times :\t [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "It was the worst of times :\t [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "It was the age of wisdom :\t [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "It was the age of foolishness :\t [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dict_vector = set()\n",
    "tokenized = [nltk.word_tokenize(i.lower()) for i in sentences]\n",
    "for i in tokenized:\n",
    "    for j in i:\n",
    "        dict_vector.add(j)\n",
    "        \n",
    "dictionary = []                                                 # thêm dần vào từ điển\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)                            # tokenize\n",
    "    for w in words:\n",
    "        if w not in dictionary:\n",
    "            dictionary.append(w)\n",
    "            \n",
    "for sent in sentences:\n",
    "    vec = [1 if w in sent else 0 for w in dictionary]\n",
    "    print(sent, ':\\t', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3"
   },
   "source": [
    "#### TF-IDF (Term frequency-Inverse Document frequency)\n",
    "\n",
    "Ngữ cảnh: \"I am very angry\" ==> \"very angry\" # tập trung vào các từ mang nhiều thông tin\n",
    "\n",
    "Đơn vị để đo thông tin trong khoa học machine-learning: entropy\n",
    "\n",
    "Đo lường tần suất ***hợp lý*** một từ (hay token) xuất hiện trong văn bản. Tần suất này được tính bằng: Mức độ xuất hiện của từ trong văn bản chia cho tỉ lệ văn bản mà từ đó xuất hiện trên tổng tất cả số lượng văn bản.\n",
    "\n",
    "Ví dụ: đối với như `what` hay `the`, các từ này xuất hiện rất nhiều tuy nhiên ko mang nhiều ý nghĩa nên cần có phương pháp loại trừ các từ này ra khỏi mô hình. Vì vậy ngoài tính toán mức độ xuất hiện của các từ trong văn bản, tuy nhiên nếu văn bản nào cũng xuất hiện từ này (hoặc đơn giản là rất nhiều > 90%) thì các từ này sẽ bị loại ra.\n",
    "\n",
    "$$ tf-idf(t, d, D)  = tf(t, d) \\dot idf(t, D)$$\n",
    "\n",
    "> `t`: từ (word hay token)\n",
    " \n",
    "> `d`: văn bản (document)\n",
    " \n",
    "> `D`: tệp các văn bản (documents)\n",
    "\n",
    "trong đó:\n",
    "\n",
    "$$ tf(t, d) = \\log(1 + freq(t, d)) $$\n",
    "$$ idf(t, D) = \\log \\left( \\dfrac{N}{count(d \\in D: t \\in d)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30bc2036-7dc6-4b33-baac-7c1ab14fe285"
   },
   "source": [
    "#### Mô hình Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "966f5ac5-3024-4991-8a7b-27ec8002ac29"
   },
   "source": [
    "*\"Word2Vec was developed at Google by Tomas Mikolov, et al. and uses Neural Networks to learn word embeddings. The beauty with word2vec is that the vectors are learned by understanding the context in which words appear. The result is vectors in which words with similar meanings end up with a similar numerical representation.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbe00fd3-483f-4343-b331-8bdbde4d8d03"
   },
   "source": [
    "**One-hot-encoding**\n",
    "\n",
    "All word are treated equal\n",
    "\n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/b3c56245-db43-48ab-b652-9ba03f4d9900.jpg?ssl=1)\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word with similar numeric value are similar in meaning\n",
    "\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/8cbfc874-3ba3-46c8-ab68-2711812ecbf1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "124e7484-2125-4bfa-8033-9f11341f6c93"
   },
   "source": [
    "Hai loại mô hình Word2Vec: **CBOW** (Continuous Bag-of-Word) và **Skip-Gram**\n",
    "\n",
    "- `Continuous Bag of Words (CBOW)`: *nhìn hình (ngữ cảnh) đoán chữ*\n",
    "\n",
    "- Ngược lại, `Skip-Gram`: *nhìn chữ đoán hình (ngữ cảnh)*\n",
    "\n",
    "![](https://i0.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/7938152f-71c8-4f28-9c25-06735e6e2b67.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03aea233-3d03-4566-8876-153325d71cd9"
   },
   "source": [
    "**Skip-gram**\n",
    "\n",
    "Window Size defines how many words before and after the target word will be used as context, typically a Window Size is 5. \n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/a8066c1d-c532-4549-bb24-19dfea5eb178_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "860004a0-d665-4d69-a4b6-41f7ef94ccea"
   },
   "source": [
    "Using a window size of 2 the input pairs for training on w(4) royal would be:\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---> Cách `Word2Vec` hoạt động: https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Khóa học thi Certificate: (`Deep Learning`): https://www.coursera.org/professional-certificates/tensorflow-in-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Công cụ dùng để Demo Visual, (dễ học), dùng để báo cáo: https://streamlit.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> --> Tạo Profile theo link Github: https://gprm.itsvg.in/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d16c807e-c950-44b4-923a-3da9c08f74e8",
    "eec9ba76-913e-4803-a39b-d6b537ed57bb",
    "a0d782c1-6302-4a8e-b8de-e936565beb95",
    "0b82a86e-d2e8-4c4c-a418-124c90218d1f",
    "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3",
    "30bc2036-7dc6-4b33-baac-7c1ab14fe285"
   ],
   "name": "nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

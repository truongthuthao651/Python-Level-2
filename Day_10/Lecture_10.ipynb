{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5869783-46cd-4ef5-8d80-b1c6eaf559bd"
   },
   "source": [
    "# X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n v√† khai ph√° d·ªØ li·ªáu vƒÉn b·∫£n (`NLP`)\n",
    "\n",
    "![](https://www.thuatngumarketing.com/wp-content/uploads/2017/12/NLP.png.pagespeed.ce_.1YNuw_5dJH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "372869b4-6785-4aaa-8abc-3e29d875fb0c"
   },
   "source": [
    "## ƒêiÃ£nh nghiÃÉa\n",
    "\n",
    "> **X·ª≠ l√≠ ng√¥n ng·ªØ t·ª± nhi√™n**: S·ª≠ d·ª•ng c√°c k·ªπ thu·∫≠t ph√¢n t√≠ch v√† l√†m s·∫°ch d·ªØ li·ªáu k·∫øt h·ª£p v·ªõi m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ khai th√°c th√¥ng tin trong d·ªØ li·ªáu ng√¥n ng·ªØ.\n",
    "\n",
    "> **D·ªØ li·ªáu ng√¥n ng·ªØ**: N√≥i cho nhau nghe (d·ªØ li·ªáu ng√¥n ng·ªØ √¢m thanh) v√† vi·∫øt cho nhau ƒë·ªçc (d·ªØ li·ªáu ng√¥n ng·ªØ vƒÉn b·∫£n)\n",
    "\n",
    "> **Text Mining**: K·ªπ thu·∫≠t x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng vƒÉn b·∫£n v√† khai th√°c th√¥ng tin t·ª´ d·ªØ li·ªáu vƒÉn b·∫£n.\n",
    "\n",
    "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0108.png)\n",
    "\n",
    "## ∆ØÃÅng duÃ£ng\n",
    "\n",
    "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0101.png)\n",
    "\n",
    "> Tham kh·∫£o: [Practical Natural Language Processing by Sowmya Vajjala, Bodhisattwa Majumder, Anuj Gupta, Harshit Surana\n",
    "](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/ch01.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da1ff90f-7126-4e50-8272-e7bb5ebc5092"
   },
   "source": [
    "## Quy triÃÄnh\n",
    "\n",
    "> 1. **Preprocessing** (Ti·ªÅn x·ª≠ l√Ω) (L√†m s·∫°ch & Chu·∫©n ho√° d·ªØ li·ªáu)\n",
    "> 2. **Tokenizing** (T√°ch t·ª´)\n",
    "> 3. **Vectorizing** (Vect∆° h√≥a)\n",
    "> 4. **Modeling** (X√¢y d·ª±ng m√¥ h√¨nh)\n",
    "> 5. **Intepreting result & Application** (·ª®ng d·ª•ng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools support us:\n",
    "> - g·ª£i √Ω vi·∫øt code (note: ch·ªâ d√πng ƒë∆∞·ª£c cho VSCode): https://copilot.github.com/\n",
    "> - code s·∫µn, c·∫£i ti·∫øn h∆°n Git_copilot: https://www.deepmind.com/blog/article/Competitive-programming-with-AlphaCode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4de5b914-9acd-4e9f-9072-a6663fea3313"
   },
   "source": [
    "## C√¥ng c·ª• v√† th∆∞ vi·ªán\n",
    "\n",
    "1. Python Nature Language Toolkit ([Python NLTK](https://www.nltk.org/)), PyVi h·ªó tr·ª£ x·ª≠ l√Ω d·ªØ li·ªáu d·∫°ng vƒÉn b·∫£n.\n",
    "2. Gensim/Tensorflow/Scikit-learn h·ªó tr·ª£ x√¢y d·ª±ng m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4da8ab72-48f2-4e8d-b889-1f8637c81797"
   },
   "source": [
    "#### 1. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu text (Text preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Today is our   last     lesson ! '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = '    Today is our   last     lesson ! '\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today', 'is', 'our', 'last', 'lesson', '!']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is our last lesson !'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(string.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is our   last     lesson !'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is our   last     lesson ! '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.lstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Today is our   last     lesson !'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    today is our   last     lesson ! '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.lower()                                                 # vi·∫øt th∆∞·ªùng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    TODAY IS OUR   LAST     LESSON ! '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.upper()                                                 # vi·∫øt in hoa to√†n b·ªô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today is great'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'today is great'.capitalize()                                  # vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu ti√™n c·ªßa c√¢u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    Today Is Our   Last     Lesson ! '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.title()                                                 # vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu c·ªßa m·ªói word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "651749cb-7a5f-4843-b816-67b5a34effde"
   },
   "source": [
    "#### TODO: L√†m s·∫°ch d·ªØ li·ªáu ch·ªâ gi·ªØ l·∫°i d·ªØ li·ªáu b·∫±ng ch·ªØ\n",
    "1. N·∫øu l√† d·ªØ li·ªáu scrapped t·ª´ web th√¨ ch·ªâ tr√≠ch l·ªçc d·ªØ li·ªáu ch·ªØ (text only)\n",
    "2. Lo·∫°i b·ªè hyperlink (n·∫øu c√≥) # https://baophapluat.com # l·∫´n th√†nh t·ª´ word \n",
    "3. N·∫øu l√† d·ªØ li·ªáu web th√¨ c·∫ßn lo·∫°i b·ªè emoji # :smile: :angry:\n",
    "4. LoaÃ£i b·ªè t·∫•t c·∫£ c√°c d·∫•u (.,\\/!@#$%^&*()+_ etc.)\n",
    "```\n",
    "r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\‚Äú\\‚Äù\\\\\\-\\:‚Ä¶&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\‚Ä¢]'\n",
    "```\n",
    "5. Lo·∫°i b·ªè t·∫•t c·∫£ c√°c s·ªë\n",
    "6. Lo·∫°i b·ªè c√°c kho·∫£ng tr·∫Øng v√† ƒë·ªïi text th√†nh lowercase # normalize # T == t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''<head></head>\n",
    "<body>\n",
    "    <p>\n",
    "    Here's 1 paragraph of text üî•. !\n",
    "    <a href=\"https://www.dataquest.io\">Learn Data Science Online ü§°</a>\n",
    "    </p>\n",
    "    <p>\n",
    "    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\n",
    "    <a href=\"https://www.python.org\">Python 101</a> </p>\n",
    "</body></html>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from bs4) (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "# 1. Tr√≠ch l·ªçc Data d·∫°ng Text kh·ªèi th·∫ª html:\n",
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><head></head>\n",
       "<body>\n",
       "<p>\n",
       "    Here's 1 paragraph of text üî•. !\n",
       "    <a href=\"https://www.dataquest.io\">Learn Data Science Online ü§°</a>\n",
       "</p>\n",
       "<p>\n",
       "    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\n",
       "    <a href=\"https://www.python.org\">Python 101</a> </p>\n",
       "</body></html>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "\n",
    "bs4.BeautifulSoup(text)                                    # ƒë·ªçc file Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n    Here's 1 paragraph of text üî•. !\\n    Learn Data Science Online ü§°\\n\\n\\n    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\\n    Python 101 \\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs4.BeautifulSoup(text).get_text('')                       # ch·ªâ l·∫•y ph·∫ßn Text ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_html(text):\n",
    "    return bs4.BeautifulSoup(text).get_text('')                       # ch·ªâ l·∫•y ph·∫ßn Text ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n    Here's 1 paragraph of text üî•. !\\n    Learn Data Science Online ü§°\\n\\n\\n    Here's a 2nd paragraph of text! Further informations at http://mci.com.vn\\n    Python 101 \\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_html(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Lo·∫°i b·ªè ƒë∆∞·ªùng Link:\n",
    "import re\n",
    "\n",
    "def del_link(text):\n",
    "    link = 'http[\\S]*'\n",
    "    text = re.sub(link, ' ', str(text))\n",
    "    text = re.sub('\\n', ' ', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       Here's 1 paragraph of text üî•. !     Learn Data Science Online ü§°       Here's a 2nd paragraph of text! Further informations at       Python 101  \""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_link(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - L√†m quen v·ªõi nh·ªØng k√≠ t·ª± trong Text: https://regex101.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: demoji in c:\\users\\hp\\anaconda3\\lib\\site-packages (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "# 3. Lo·∫°i b·ªè Emoji:\n",
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import demoji\n",
    "\n",
    "def del_emoji(text):\n",
    "    return demoji.replace(text, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"       Here's 1 paragraph of text . !     Learn Data Science Online        Here's a 2nd paragraph of text! Further informations at       Python 101  \""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_emoji(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Lo·∫°i b·ªè d·∫•u:\n",
    "def del_punctuation(text):\n",
    "    Punc = r'[\\,\\.\\/\\\\\\!\\@\\#\\+\\\"\\'\\;\\)\\(\\‚Äú\\‚Äù\\\\\\-\\:‚Ä¶&><=\\-\\%\\|\\^\\$\\&\\)\\(\\[\\]\\{\\}\\?\\*\\‚Ä¢]'\n",
    "    text = re.sub(Punc, ' ', str(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       Here s 1 paragraph of text         Learn Data Science Online        Here s a 2nd paragraph of text  Further informations at       Python 101  '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_punctuation(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Lo·∫°i b·ªè s·ªë:\n",
    "def del_numbers(text):\n",
    "    return re.sub('\\d+', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'       Here s   paragraph of text         Learn Data Science Online        Here s a  nd paragraph of text  Further informations at       Python    '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_numbers(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Lo·∫°i b·ªè c√°c kho·∫£ng tr·∫Øng v√† ƒë·ªïi text th√†nh lowercase # normalize # T == t:\n",
    "def del_space(text):\n",
    "    return re.sub('\\s+', ' ', text).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'here s paragraph of text learn data science online here s a nd paragraph of text further informations at python'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = del_space(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Trong Vectors: Num != num != NUm != NUM --> n√™n chuy·ªÉn v·ªÅ d·∫°ng vi·∫øt th∆∞·ªùng (lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. T√°ch t·ª´ (Tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " ' ',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 'a',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'p',\n",
       " 'h',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " 'l',\n",
       " 'e',\n",
       " 'a',\n",
       " 'r',\n",
       " 'n',\n",
       " ' ',\n",
       " 'd',\n",
       " 'a',\n",
       " 't',\n",
       " 'a',\n",
       " ' ',\n",
       " 's',\n",
       " 'c',\n",
       " 'i',\n",
       " 'e',\n",
       " 'n',\n",
       " 'c',\n",
       " 'e',\n",
       " ' ',\n",
       " 'o',\n",
       " 'n',\n",
       " 'l',\n",
       " 'i',\n",
       " 'n',\n",
       " 'e',\n",
       " ' ',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " 'e',\n",
       " ' ',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " ' ',\n",
       " 'n',\n",
       " 'd',\n",
       " ' ',\n",
       " 'p',\n",
       " 'a',\n",
       " 'r',\n",
       " 'a',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'p',\n",
       " 'h',\n",
       " ' ',\n",
       " 'o',\n",
       " 'f',\n",
       " ' ',\n",
       " 't',\n",
       " 'e',\n",
       " 'x',\n",
       " 't',\n",
       " ' ',\n",
       " 'f',\n",
       " 'u',\n",
       " 'r',\n",
       " 't',\n",
       " 'h',\n",
       " 'e',\n",
       " 'r',\n",
       " ' ',\n",
       " 'i',\n",
       " 'n',\n",
       " 'f',\n",
       " 'o',\n",
       " 'r',\n",
       " 'm',\n",
       " 'a',\n",
       " 't',\n",
       " 'i',\n",
       " 'o',\n",
       " 'n',\n",
       " 's',\n",
       " ' ',\n",
       " 'a',\n",
       " 't',\n",
       " ' ',\n",
       " 'p',\n",
       " 'y',\n",
       " 't',\n",
       " 'h',\n",
       " 'o',\n",
       " 'n']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_char(text):                             # character tokenization\n",
    "    return [char for char in text]\n",
    "\n",
    "split_char(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 's',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'science',\n",
       " 'online',\n",
       " 'here',\n",
       " 's',\n",
       " 'a',\n",
       " 'nd',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'further',\n",
       " 'informations',\n",
       " 'at',\n",
       " 'python']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split()                                           # word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (4.47.0)\n",
      "Requirement already satisfied: click in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (0.16.0)\n",
      "Requirement already satisfied: regex in c:\\users\\hp\\anaconda3\\lib\\site-packages (from nltk) (2020.6.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['here',\n",
       " 's',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'learn',\n",
       " 'data',\n",
       " 'science',\n",
       " 'online',\n",
       " 'here',\n",
       " 's',\n",
       " 'a',\n",
       " 'nd',\n",
       " 'paragraph',\n",
       " 'of',\n",
       " 'text',\n",
       " 'further',\n",
       " 'informations',\n",
       " 'at',\n",
       " 'python']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.word_tokenize(text)                                                          # word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Today is our last lesson.', 'Wish you all happy learning!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.sent_tokenize('Today is our last lesson. Wish you all happy learning!')                                   # sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H·ªçc', 'sinh', 'h·ªçc', 'sinh', 'h·ªçc']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1 = 'H·ªçc sinh h·ªçc sinh h·ªçc'\n",
    "nltk.word_tokenize(text1)                                # nh·ªØng t·ª´ 'h·ªçc' & 'sinh' mang nhi·ªÅu nghƒ©a kh√°c nhau nh∆∞ng m√°y s·∫Ω kh√¥ng hi·ªÉu v√† d·ªÖ b·ªã ƒë√°nh gi√° sai     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - C√°ch `tokenize` ƒë·ªëi v·ªõi data b·∫±ng Vietnamese:  [Underthesea](https://github.com/undertheseanlp/underthesea), or [Pyvi](https://github.com/trungtv/pyvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvi in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyvi) (1.0.1)\n",
      "Requirement already satisfied: sklearn-crfsuite in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyvi) (0.3.6)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (0.16.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (2.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from scikit-learn->pyvi) (1.22.2)\n",
      "Requirement already satisfied: python-crfsuite>=0.8.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.9.7)\n",
      "Requirement already satisfied: six in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
      "Requirement already satisfied: tabulate in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
      "Requirement already satisfied: tqdm>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from sklearn-crfsuite->pyvi) (4.47.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H·ªçc_sinh ph·∫£i h·ªçc_sinh_h·ªçc'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyvi.ViTokenizer import tokenize\n",
    "tokenize('H·ªçc sinh ph·∫£i h·ªçc sinh h·ªçc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T·ªïng h·ª£p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):                                                    # l√†m s·∫°ch d·ªØ li·ªáu t·ª´ ƒë·∫ßu t·ªõi cu·ªëi\n",
    "    text = del_html(text)\n",
    "    text = del_link(text)\n",
    "    text = del_numbers(text)\n",
    "    text = del_emoji(text)\n",
    "    text = del_punctuation(text)\n",
    "    text = del_space(text)\n",
    "    return tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "      <td>(T·ªï Qu·ªëc) - V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞...</td>\n",
       "      <td>S√°ng ng√†y 9/7, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...</td>\n",
       "      <td>T·∫°i cu·ªôc h·ªçp H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ch...</td>\n",
       "      <td>Tham d·ª± cu·ªôc h·ªçp c√≥ Th·ªëng ƒë·ªëc NHNN L√™ Minh H∆∞n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...</td>\n",
       "      <td>Chuy√™n gia ƒë·ªÅ xu·∫•t do d·ªãch b·ªánh Covid-19 c√≤n k...</td>\n",
       "      <td>S√°ng 9/7, Th·ªß t∆∞·ªõng Nguy·ªÖn Xu√¢n Ph√∫c ch·ªß tr√¨ h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "      <td>V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...</td>\n",
       "      <td>V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...</td>\n",
       "      <td>ƒê·ªông th√°i th·∫Øt ch·∫∑t hay n·ªõi l·ªèng c·ªßa ch√≠nh s√°c...</td>\n",
       "      <td>L·ªùi t√≤a so·∫°n Th·ªã tr∆∞·ªùng b·∫•t ƒë·ªông s·∫£n ng√†y c√†ng...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...   \n",
       "1  Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...   \n",
       "2  ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...   \n",
       "3  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...   \n",
       "4  T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  (T·ªï Qu·ªëc) - V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞...   \n",
       "1  T·∫°i cu·ªôc h·ªçp H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ch...   \n",
       "2  Chuy√™n gia ƒë·ªÅ xu·∫•t do d·ªãch b·ªánh Covid-19 c√≤n k...   \n",
       "3  V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...   \n",
       "4  ƒê·ªông th√°i th·∫Øt ch·∫∑t hay n·ªõi l·ªèng c·ªßa ch√≠nh s√°c...   \n",
       "\n",
       "                                             content  \n",
       "0  S√°ng ng√†y 9/7, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ...  \n",
       "1  Tham d·ª± cu·ªôc h·ªçp c√≥ Th·ªëng ƒë·ªëc NHNN L√™ Minh H∆∞n...  \n",
       "2  S√°ng 9/7, Th·ªß t∆∞·ªõng Nguy·ªÖn Xu√¢n Ph√∫c ch·ªß tr√¨ h...  \n",
       "3  V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...  \n",
       "4  L·ªùi t√≤a so·∫°n Th·ªã tr∆∞·ªùng b·∫•t ƒë·ªông s·∫£n ng√†y c√†ng...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('https://github.com/honghanhh/mci_python_36a1_l2/raw/main/Lectures/lecture_09/data.json')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>snippet</th>\n",
       "      <th>content</th>\n",
       "      <th>corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "      <td>(T·ªï Qu·ªëc) - V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞...</td>\n",
       "      <td>S√°ng ng√†y 9/7, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ...</td>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...</td>\n",
       "      <td>T·∫°i cu·ªôc h·ªçp H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ch...</td>\n",
       "      <td>Tham d·ª± cu·ªôc h·ªçp c√≥ Th·ªëng ƒë·ªëc NHNN L√™ Minh H∆∞n...</td>\n",
       "      <td>Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...</td>\n",
       "      <td>Chuy√™n gia ƒë·ªÅ xu·∫•t do d·ªãch b·ªánh Covid-19 c√≤n k...</td>\n",
       "      <td>S√°ng 9/7, Th·ªß t∆∞·ªõng Nguy·ªÖn Xu√¢n Ph√∫c ch·ªß tr√¨ h...</td>\n",
       "      <td>ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "      <td>V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...</td>\n",
       "      <td>V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...</td>\n",
       "      <td>Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...</td>\n",
       "      <td>ƒê·ªông th√°i th·∫Øt ch·∫∑t hay n·ªõi l·ªèng c·ªßa ch√≠nh s√°c...</td>\n",
       "      <td>L·ªùi t√≤a so·∫°n Th·ªã tr∆∞·ªùng b·∫•t ƒë·ªông s·∫£n ng√†y c√†ng...</td>\n",
       "      <td>T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...   \n",
       "1  Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...   \n",
       "2  ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...   \n",
       "3  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...   \n",
       "4  T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...   \n",
       "\n",
       "                                             snippet  \\\n",
       "0  (T·ªï Qu·ªëc) - V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞...   \n",
       "1  T·∫°i cu·ªôc h·ªçp H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ch...   \n",
       "2  Chuy√™n gia ƒë·ªÅ xu·∫•t do d·ªãch b·ªánh Covid-19 c√≤n k...   \n",
       "3  V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...   \n",
       "4  ƒê·ªông th√°i th·∫Øt ch·∫∑t hay n·ªõi l·ªèng c·ªßa ch√≠nh s√°c...   \n",
       "\n",
       "                                             content  \\\n",
       "0  S√°ng ng√†y 9/7, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s√°ch t√†i ...   \n",
       "1  Tham d·ª± cu·ªôc h·ªçp c√≥ Th·ªëng ƒë·ªëc NHNN L√™ Minh H∆∞n...   \n",
       "2  S√°ng 9/7, Th·ªß t∆∞·ªõng Nguy·ªÖn Xu√¢n Ph√∫c ch·ªß tr√¨ h...   \n",
       "3  V·ªõi b·ªëi c·∫£nh hi·ªán nay, H·ªôi ƒë·ªìng T∆∞ v·∫•n ch√≠nh s...   \n",
       "4  L·ªùi t√≤a so·∫°n Th·ªã tr∆∞·ªùng b·∫•t ƒë·ªông s·∫£n ng√†y c√†ng...   \n",
       "\n",
       "                                              corpus  \n",
       "0  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...  \n",
       "1  Th·ªëng nh·∫•t k·ªãch b·∫£n tƒÉng tr∆∞·ªüng 3-4%, l·∫°m ph√°t...  \n",
       "2  ƒê·ªÅ xu·∫•t tƒÉng li·ªÅu l∆∞·ª£ng g√≥i k√≠ch th√≠ch kinh t·∫ø...  \n",
       "3  Th·ªß t∆∞·ªõng: Kh√°c v·ªõi ƒëa s·ªë c√°c n∆∞·ªõc, d∆∞ ƒë·ªãa ch√≠...  \n",
       "4  T√°c ƒë·ªông t·ª´ ch√≠nh s√°ch t√≠n d·ª•ng: BƒêS c√≥ th·ªÉ ƒë√≥...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data.apply(lambda x: ' '.join(x), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'th·ªß_t∆∞·ªõng kh√°c v·ªõi ƒëa_s·ªë c√°c n∆∞·ªõc d∆∞ ƒë·ªãa_ch√≠nh_s√°ch t√†i kh√≥a ti·ªÅn_t·ªá c·ªßa vi·ªát nam c√≤n kh√° l·ªõn t·ªï_qu·ªëc v·ªõi b·ªëi_c·∫£nh hi·ªán_nay h·ªôi_ƒë·ªìng t∆∞_v·∫•n ch√≠nh_s√°ch t√†i_ch√≠nh ti·ªÅn_t·ªá qu·ªëc_gia th·ªëng_nh·∫•t k·ªãch_b·∫£n tƒÉng_tr∆∞·ªüng t·ª´ ki·ªÉm_so√°t l·∫°m_ph√°t d∆∞·ªõi nƒÉm v√† ƒë·∫ßu tƒÉng_tr∆∞·ªüng t√≠n_d·ª•ng tr√™n ch·ªß_tr∆∞∆°ng tƒÉng th√™m b·ªôi_chi ng√¢n_s√°ch n·ª£ c√¥ng kho·∫£ng gdp s√°ng ng√†y h·ªôi_ƒë·ªìng t∆∞_v·∫•n ch√≠nh_s√°ch t√†i_ch√≠nh ti·ªÅn_t·ªá qu·ªëc_gia ƒë√£ h·ªçp d∆∞·ªõi s·ª± ch·ªß_tr√¨ c·ªßa th·ªß_t∆∞·ªõng nguy·ªÖn xu√¢n ph√∫c ch·ªß_t·ªãch h·ªôi_ƒë·ªìng d∆∞ ƒë·ªãa_ch√≠nh_s√°ch t√†i kh√≥a ti·ªÅn_t·ªá c√≤n l·ªõn th·ªß_t∆∞·ªõng nguy·ªÖn xu√¢n ph√∫c cho r·∫±ng d·ªãch_b·ªánh covid b√πng_ph√°t di·ªán r·ªông tr√™n to√†n_c·∫ßu di·ªÖn_bi·∫øn ph·ª©c_t·∫°p ch∆∞a d·ª´ng l·∫°i nh·∫•t_l√† t·∫°i c√°c ƒë·ªëi_t√°c l·ªõn c·ªßa n∆∞·ªõc ta kinh_t·∫ø to√†n_c·∫ßu s·ª•t_gi·∫£m m·∫°nh nhi·ªÅu t·ªï_ch·ª©c qu·ªëc_t·∫ø d·ª±_b√°o kinh_t·∫ø th·∫ø_gi·ªõi tƒÉng_tr∆∞·ªüng √¢m trong nƒÉm nay trong b·ªëi_c·∫£nh ƒë√≥ h·∫ßu_h·∫øt c√°c n∆∞·ªõc ƒë·ªÅu n·ªõi l·ªèng ch√≠nh_s√°ch t√†i_ch√≠nh ti·ªÅn_t·ªá v·ªõi m·ª©c_ƒë·ªô ch∆∞a t·ª´ng c√≥ theo th·ªëng_k√™ m·ªõi nh·∫•t n·∫øu th√°ng t·ªïng_c√°c g√≥i k√≠ch_th√≠ch t√†i kh√≥a m·ªõi l√† t·ª∑ usd th√¨ ƒë·∫øn nay ƒë√£ tƒÉng l√™n t·ª∑ usd ch∆∞a c√≥ d·∫•u_hi·ªáu d·ª´ng l·∫°i v·ªÅ t√¨nh_h√¨nh trong n∆∞·ªõc theo th·ªß_t∆∞·ªõng ch√∫ng_ta l√† m·ªôt trong nh·ªØng n∆∞·ªõc ki·ªÉm_so√°t d·ªãch_b·ªánh s·ªõm nh·∫•t g·∫ßn th√°ng qua kh√¥ng c√≥ ca l√¢y_nhi·ªÖm m·ªõi trong c·ªông_ƒë·ªìng v√† ƒëang t√≠ch_c·ª±c th·ª±c_hi·ªán m·ª•c_ti√™u k√©p hi·ªán_nay c√°c c√¢n_ƒë·ªëi l·ªõn c·ªßa n·ªÅn kinh_t·∫ø ƒë·ªÅu gi·ªØ v·ªØng c√°c lƒ©nh_v·ª±c tr·ªçng_y·∫øu c√≥ xu_h∆∞·ªõng ƒëi l√™n m·∫°nh_m·∫Ω tuy_nhi√™n th·ªß_t∆∞·ªõng l∆∞u_√Ω c·∫ßn nh·∫≠n_di·ªán r√µ r·ªßi_ro ƒë∆∞a ra c√°c bi·ªán_ph√°p ƒëi·ªÅu_h√†nh ƒë·ªìng_b·ªô hi·ªáu_qu·∫£ h∆°n ƒë·ªÉ gi·ªØ ·ªïn_ƒë·ªãnh vƒ©_m√¥ ki·ªÉm_so√°t l·∫°m_ph√°t d∆∞·ªõi kh√°c v·ªõi ƒëa_s·ªë c√°c n∆∞·ªõc d∆∞ ƒë·ªãa_ch√≠nh_s√°ch t√†i kh√≥a ti·ªÅn_t·ªá c·ªßa ch√∫ng_ta c√≤n kh√° l·ªõn cho k√≠ch_th√≠ch t·ªïng_c·∫ßu th√∫c_ƒë·∫©y tƒÉng_tr∆∞·ªüng c·∫ßn ch√≠nh_s√°ch ph·ª•c_h·ªìi ph√°t_tri·ªÉn kinh_t·∫ø d√†i_h·∫°n cho c·∫£ nƒÉm t·∫°i cu·ªôc h·ªçp c√°c th√†nh_vi√™n h·ªôi_ƒë·ªìng nh·∫≠n_ƒë·ªãnh t√¨nh_h√¨nh_th·∫ø_gi·ªõi khu_v·ª±c ti·∫øp_t·ª•c di·ªÖn_bi·∫øn ph·ª©c_t·∫°p kh√≥ l∆∞·ªùng d·ªãch_b·ªánh ch∆∞a k·∫øt_th√∫c trong nƒÉm nay m√† c√≥_th·ªÉ k√©o_d√†i trong th·ªùi_gian t·ªõi do v·∫≠y c√≥ √Ω_ki·∫øn cho r·∫±ng g√≥i ch√≠nh_s√°ch ph·ª•c_h·ªìi ph√°t_tri·ªÉn kinh_t·∫ø ph·∫£i mang t√≠nh d√†i_h·∫°n cho c·∫£ nƒÉm ch·ª© kh√¥ng ch·ªâ trong nƒÉm nay v·ªõi tinh_th·∫ßn ti·∫øn_c√¥ng kinh_t·∫ø trong khi ph√≤ng_th·ªß d·ªãch_b·ªánh chuy√™n_gia b√πi ƒë·ª©c th·ª• cho r·∫±ng c·ª©u_tr·ª£ doanh_nghi·ªáp l√† vi·ªác c·∫ßn l√†m nh∆∞ng gia_tƒÉng s·ª©c_s·ªëng cho doanh_nghi·ªáp kh√¥ng_th·ªÉ ch·ªâ l√†m trong nƒÉm ngo√†i c∆°_ch·∫ø h·ªó_tr·ª£ kh·∫Øc_ph·ª•c ·∫£nh_h∆∞·ªüng c·ªßa covid c·∫ßn c√≥ c√°c ch√≠nh_s√°ch h·ªó_tr·ª£ ƒë·∫∑c_bi·ªát ph·ª•c_v·ª• t√°i_c∆°_c·∫•u doanh_nghi·ªáp m·∫°nh h∆°n kh√¥ng ch·ªâ t√°i_c∆°_c·∫•u th·ªã_tr∆∞·ªùng ƒë·∫ßu_v√†o ƒë·∫ßu_t∆∞ m√† c·∫£ lao_ƒë·ªông tƒÉng s·ª©c ch·ªëng ch·ªãu c·ªßa n·ªÅn kinh_t·∫ø th·ª±c_l·ª±c doanh_nghi·ªáp c·ªßa ch√∫ng_ta c√≤n y·∫øu ƒë·∫∑c_bi·ªát ƒëa_s·ªë l√† doanh_nghi·ªáp v·ª´a v√† nh·ªè ts tr·∫ßn ƒë√¨nh thi√™n nh·∫≠n_ƒë·ªãnh do ƒë√≥ kh√¥ng ch·ªâ c·ª©u c√°i c≈© m√† c√≤n c·∫ßn t·∫°o c·∫£ c√°i m·ªõi t·ª©c_l√† b√™n c·∫°nh h·ªó_tr·ª£ doanh_nghi·ªáp c·∫ßn th√∫c_ƒë·∫©y kh·ªüi_nghi·ªáp s√°ng_t·∫°o trong t√¨nh_th·∫ø kh√≥_khƒÉn n√†y c·∫ßn t√≠nh t·ªõi c√°c bi·ªán_ph√°p x·ª≠_l√Ω n·ª£ x·∫•u ts tr·∫ßn ƒë√¨nh thi√™n ƒë·ªÅ_ngh·ªã ch√≠nh_ph·ªß nh√†_n∆∞·ªõc ph·∫£i l√† ng∆∞·ªùi mua h√†ng l·ªõn nh·∫•t ƒë·ªëi_v·ªõi c√°c s·∫£n_ph·∫©m made in vi·ªát nam m·ªôt_s·ªë th√†nh_vi√™n ki·∫øn_ngh·ªã so v·ªõi c√°c n∆∞·ªõc th√¨ g√≥i h·ªó_tr·ª£ t√†i kh√≥a c·ªßa ch√∫ng_ta l√† √≠t_nh·∫•t do ƒë√≥ c·∫ßn t·∫≠p_trung v√†o g√≥i n√†y nhi·ªÅu h∆°n c≈©ng nh∆∞ tƒÉng quy_m√¥ c√°c g√≥i h·ªó_tr·ª£ nh·∫•t l√† cho ng√†nh_h√†ng kh√¥ng_k√≠ch_c·∫ßu n·ªôi_ƒë·ªãa c≈©ng_n√™n h∆∞·ªõng v√†o k√≠ch_c·∫ßu du_l·ªãch b√°n_l·∫ª t√≠n_d·ª•ng ti√™u_d√πng ng√†nh ng√¢n_h√†ng c·∫ßn ti·∫øp_t·ª•c h·∫°_l√£i_su·∫•t cho vay h·ªôi_ƒë·ªìng t∆∞_v·∫•n ch√≠nh_s√°ch t√†i_ch√≠nh ti·ªÅn_t·ªá qu·ªëc_gia ƒë√£ th·ªëng_nh·∫•t k·ªãch_b·∫£n tƒÉng_tr∆∞·ªüng t·ª´ ki·ªÉm_so√°t l·∫°m_ph√°t d∆∞·ªõi nƒÉm v√† ƒë·∫ßu tƒÉng_tr∆∞·ªüng t√≠n_d·ª•ng tr√™n ch·ªß_tr∆∞∆°ng tƒÉng th√™m b·ªôi_chi ng√¢n_s√°ch n·ª£ c√¥ng kho·∫£ng gdp ƒë·ªÉ c√≥ th√™m ngu·ªìn_l·ª±c ch√∫ng_ta chu·∫©n_b·ªã s·∫µn_s√†ng ƒë·ªÉ c√≥_th·ªÉ h·ªó_tr·ª£ doanh_nghi·ªáp ki√™n_quy·∫øt b·∫£o_v·ªá h·ªá_th·ªëng doanh_nghi·ªáp kh√¥ng ƒë·ªÉ ƒë·ª©t_g√£y m·∫•t nƒÉng_l·ª±c s·∫£n_xu·∫•t trong nh·ªØng ng√†nh lƒ©nh_v·ª±c tr·ªçng_y·∫øu ng√†nh ng√¢n_h√†ng ti·∫øp_t·ª•c xem_x√©t gi·∫£m l√£i_su·∫•t chia_s·∫ª kh√≥_khƒÉn v·ªõi doanh_nghi·ªáp v√† ng∆∞·ªùi d√¢n ti·∫øp_t·ª•c ti·∫øt_ki·ªám chi th∆∞·ªùng_xuy√™n c·∫Øt_gi·∫£m c√°c kho·∫£n chi_h·ªôi_ngh·ªã h·ªôi_th·∫£o d√†nh ngu·ªìn_l·ª±c cho nh·ªØng nhi·ªám_v·ª• c·∫•p_b√°ch ch·ªëng d·ªãch h·ªó_tr·ª£ k·ªãp_th·ªùi cho ng∆∞·ªùi d√¢n doanh_nghi·ªáp'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['corpus'] = data['corpus'].apply(clean_text)                   # corpora l√† corpus: sdung trong Text Mining (th·ªÉ hi·ªán vi·ªác sdung nhi·ªÅu vƒÉn b·∫£n Text)\n",
    "data['corpus'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Read more about Tokenization: https://huggingface.co/docs/transformers/main_classes/tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa513f1e-0125-4cdc-a268-883af940025b"
   },
   "source": [
    "#### 3. M√£ h√≥a d·ªØ li·ªáu text (Word embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Trong `CountVectorizer` s·∫Ω x·ª≠ l√Ω data c∆° b·∫£n, filltering 1 s·ªë t·ª´ `stopwords` trong ti·∫øng Anh, ƒë·ªëi v·ªõi Vietnamese th√¨ ch√∫ng ta ph·∫£i t·ª± lo·∫°i b·ªè\n",
    "> - `Stopwords` l√† nh·ªØng t·ª´ kh√¥ng c√≥ √Ω nghƒ©a, mang t√≠nh ch·∫•t l√†m cho ƒë√∫ng ng·ªØ ph√°p trong c√¢u, ex: `the, is, about, ...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data['corpus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876, 5743)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_counts.shape                                                       # c√≥ d·∫°ng (s·ªë rows, s·ªë Tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `TF-IDF` (Term Frequency & Inverse Document Frequency):\n",
    "- `TF`: M·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa t·ª´ trong vƒÉn b·∫£n\n",
    "- `IDF`: chia cho t·ªâ l·ªá vƒÉn b·∫£n m√† t·ª´ ƒë√≥ xu·∫•t hi·ªán tr√™n t·ªïng t·∫•t c·∫£ s·ªë l∆∞·ª£ng vƒÉn b·∫£n\n",
    "---> Scale ƒë∆∞·ª£c gi√° tr·ªã Vectors v·ªÅ 1 kho·∫£ng gi√° tr·ªã nh·∫•t ƒë·ªãnh\n",
    "---> Lo·∫°i b·ªè nh·ªØng t·ª´ ƒë∆∞·ª£c coi l√† `Stopwords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tf_transformer = TfidfTransformer()\n",
    "X_train_tf = tf_transformer.fit(X_train_counts).transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(876, 5743)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b347065-59af-414d-8fee-f37389be5b5d"
   },
   "source": [
    "### Khai ph√° d·ªØ li·ªáu text (Text Mining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It was the best of times',\n",
       " 'It was the worst of times',\n",
       " 'It was the age of wisdom',\n",
       " 'It was the age of foolishness']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = [\n",
    "    'It was the best of times',                             # best times\n",
    "    'It was the worst of times',                            # worst times\n",
    "    'It was the age of wisdom',                             # age wisdom\n",
    "    'It was the age of foolishness'                         # age foolishness\n",
    "]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b82a86e-d2e8-4c4c-a418-124c90218d1f"
   },
   "source": [
    "#### Kh√°i ni·ªám Bag-of-Words\n",
    "\n",
    "L√† k·ªπ thu·∫≠t chia vƒÉn b·∫£n th√†nh c√°c t·ªï h·ª£p t·ª´ kh√°c nhau (b·∫±ng ph∆∞∆°ng ph√°p tokenize). C√°ch chia ph·ªï bi·∫øn l√† m·ªói c√¢u th√†nh 1 vƒÉn b·∫£n (bag) v√† m·ªói vƒÉn b·∫£n ƒë∆∞·ª£c chia th√†nh t·ª´ (word). D·ª±a v√†o ƒë√≥ c√≥ th·ªÉ ƒëo l∆∞·ªùng m·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa c√°c t·ª´ trong vƒÉn b·∫£n v√† x√¢y d·ª±ng m·ªëi li√™n h·ªá gi·ªØa ng·ªØ c·∫£nh v√† c√°c t·ª´. \n",
    "\n",
    "Hai y·∫øu t·ªë:\n",
    "1. T·ª´ ƒëi·ªÉn c·ªßa c√°c t·ª´ ƒë∆∞·ª£c s·ª≠ d·ª•ng\n",
    "2. M·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa c√°c t·ª´ trong t·ª´ ƒëi·ªÉn\n",
    "*M·ªói t·ª´ hay token ƒë∆∞·ª£c g·ªçi l√† m·ªôt `gram`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1637329798399,
     "user": {
      "displayName": "Samuel Doan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjbcdQ40FcyCSIyitBN2ptY3zHWA_09harKPYF2=s64",
      "userId": "04384888964432338542"
     },
     "user_tz": -420
    },
    "id": "vKTTqu9ccnHq",
    "outputId": "b3daeb63-d877-4c17-b270-e435c4c76917"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was the best of times :\t [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "It was the worst of times :\t [1, 1, 1, 0, 1, 1, 1, 0, 0, 0]\n",
      "It was the age of wisdom :\t [1, 1, 1, 0, 1, 0, 0, 1, 1, 0]\n",
      "It was the age of foolishness :\t [1, 1, 1, 0, 1, 0, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "dict_vector = set()\n",
    "tokenized = [nltk.word_tokenize(i.lower()) for i in sentences]\n",
    "for i in tokenized:\n",
    "    for j in i:\n",
    "        dict_vector.add(j)\n",
    "        \n",
    "dictionary = []                                                 # th√™m d·∫ßn v√†o t·ª´ ƒëi·ªÉn\n",
    "for sent in sentences:\n",
    "    words = nltk.word_tokenize(sent)                            # tokenize\n",
    "    for w in words:\n",
    "        if w not in dictionary:\n",
    "            dictionary.append(w)\n",
    "            \n",
    "for sent in sentences:\n",
    "    vec = [1 if w in sent else 0 for w in dictionary]\n",
    "    print(sent, ':\\t', vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3"
   },
   "source": [
    "#### TF-IDF (Term frequency-Inverse Document frequency)\n",
    "\n",
    "Ng·ªØ c·∫£nh: \"I am very angry\" ==> \"very angry\" # t·∫≠p trung v√†o c√°c t·ª´ mang nhi·ªÅu th√¥ng tin\n",
    "\n",
    "ƒê∆°n v·ªã ƒë·ªÉ ƒëo th√¥ng tin trong khoa h·ªçc machine-learning: entropy\n",
    "\n",
    "ƒêo l∆∞·ªùng t·∫ßn su·∫•t ***h·ª£p l√Ω*** m·ªôt t·ª´ (hay token) xu·∫•t hi·ªán trong vƒÉn b·∫£n. T·∫ßn su·∫•t n√†y ƒë∆∞·ª£c t√≠nh b·∫±ng: M·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa t·ª´ trong vƒÉn b·∫£n chia cho t·ªâ l·ªá vƒÉn b·∫£n m√† t·ª´ ƒë√≥ xu·∫•t hi·ªán tr√™n t·ªïng t·∫•t c·∫£ s·ªë l∆∞·ª£ng vƒÉn b·∫£n.\n",
    "\n",
    "V√≠ d·ª•: ƒë·ªëi v·ªõi nh∆∞ `what` hay `the`, c√°c t·ª´ n√†y xu·∫•t hi·ªán r·∫•t nhi·ªÅu tuy nhi√™n ko mang nhi·ªÅu √Ω nghƒ©a n√™n c·∫ßn c√≥ ph∆∞∆°ng ph√°p lo·∫°i tr·ª´ c√°c t·ª´ n√†y ra kh·ªèi m√¥ h√¨nh. V√¨ v·∫≠y ngo√†i t√≠nh to√°n m·ª©c ƒë·ªô xu·∫•t hi·ªán c·ªßa c√°c t·ª´ trong vƒÉn b·∫£n, tuy nhi√™n n·∫øu vƒÉn b·∫£n n√†o c≈©ng xu·∫•t hi·ªán t·ª´ n√†y (ho·∫∑c ƒë∆°n gi·∫£n l√† r·∫•t nhi·ªÅu > 90%) th√¨ c√°c t·ª´ n√†y s·∫Ω b·ªã lo·∫°i ra.\n",
    "\n",
    "$$ tf-idf(t, d, D)  = tf(t, d) \\dot idf(t, D)$$\n",
    "\n",
    "> `t`: t·ª´ (word hay token)\n",
    " \n",
    "> `d`: vƒÉn b·∫£n (document)\n",
    " \n",
    "> `D`: t·ªáp c√°c vƒÉn b·∫£n (documents)\n",
    "\n",
    "trong ƒë√≥:\n",
    "\n",
    "$$ tf(t, d) = \\log(1 + freq(t, d)) $$\n",
    "$$ idf(t, D) = \\log \\left( \\dfrac{N}{count(d \\in D: t \\in d)} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30bc2036-7dc6-4b33-baac-7c1ab14fe285"
   },
   "source": [
    "#### M√¥ h√¨nh Word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "966f5ac5-3024-4991-8a7b-27ec8002ac29"
   },
   "source": [
    "*\"Word2Vec was developed at Google by Tomas Mikolov, et al. and uses Neural Networks to learn word embeddings. The beauty with word2vec is that the vectors are learned by understanding the context in which words appear. The result is vectors in which words with similar meanings end up with a similar numerical representation.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dbe00fd3-483f-4343-b331-8bdbde4d8d03"
   },
   "source": [
    "**One-hot-encoding**\n",
    "\n",
    "All word are treated equal\n",
    "\n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/b3c56245-db43-48ab-b652-9ba03f4d9900.jpg?ssl=1)\n",
    "\n",
    "**Word2Vec**\n",
    "\n",
    "Word with similar numeric value are similar in meaning\n",
    "\n",
    "![](https://i1.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/8cbfc874-3ba3-46c8-ab68-2711812ecbf1.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "124e7484-2125-4bfa-8033-9f11341f6c93"
   },
   "source": [
    "Hai lo·∫°i m√¥ h√¨nh Word2Vec: **CBOW** (Continuous Bag-of-Word) v√† **Skip-Gram**\n",
    "\n",
    "- `Continuous Bag of Words (CBOW)`: *nh√¨n h√¨nh (ng·ªØ c·∫£nh) ƒëo√°n ch·ªØ*\n",
    "\n",
    "- Ng∆∞·ª£c l·∫°i, `Skip-Gram`: *nh√¨n ch·ªØ ƒëo√°n h√¨nh (ng·ªØ c·∫£nh)*\n",
    "\n",
    "![](https://i0.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/7938152f-71c8-4f28-9c25-06735e6e2b67.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03aea233-3d03-4566-8876-153325d71cd9"
   },
   "source": [
    "**Skip-gram**\n",
    "\n",
    "Window Size defines how many words before and after the target word will be used as context, typically a Window Size is 5. \n",
    "![](https://i2.wp.com/insightbot.blob.core.windows.net/blogimagecontainer/a8066c1d-c532-4549-bb24-19dfea5eb178_med.jpg?ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "860004a0-d665-4d69-a4b6-41f7ef94ccea"
   },
   "source": [
    "Using a window size of 2 the input pairs for training on w(4) royal would be:\n",
    "![](https://israelg99.github.io/images/2017-03-23-Word2Vec-Explained/training_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ---> C√°ch `Word2Vec` ho·∫°t ƒë·ªông: https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - Kh√≥a h·ªçc thi Certificate: (`Deep Learning`): https://www.coursera.org/professional-certificates/tensorflow-in-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> - C√¥ng c·ª• d√πng ƒë·ªÉ Demo Visual, (d·ªÖ h·ªçc), d√πng ƒë·ªÉ b√°o c√°o: https://streamlit.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> --> T·∫°o Profile theo link Github: https://gprm.itsvg.in/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "d16c807e-c950-44b4-923a-3da9c08f74e8",
    "eec9ba76-913e-4803-a39b-d6b537ed57bb",
    "a0d782c1-6302-4a8e-b8de-e936565beb95",
    "0b82a86e-d2e8-4c4c-a418-124c90218d1f",
    "3645f46c-63e0-4a38-a1f5-ad8ab78b8aa3",
    "30bc2036-7dc6-4b33-baac-7c1ab14fe285"
   ],
   "name": "nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
